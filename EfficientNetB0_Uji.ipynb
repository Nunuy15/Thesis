{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOvvc6JuhQCLJrD8dH1Q5Kt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nunuy15/Thesis/blob/main/EfficientNetB0_Uji.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W7AKkH_uqpxm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d02ceeb8-91b4-4d9e-ac05-46583ffa5fa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Directories\n",
        "train_dir = '/content/drive/MyDrive/DATASET_FFB/SUHARJITO/NORMAL/DATASET_PHOTOMETRIC/train'\n",
        "val_dir = '/content/drive/MyDrive/DATASET_FFB/SUHARJITO/NORMAL/DATASET_PHOTOMETRIC/valid'\n",
        "test_dir = '/content/drive/MyDrive/DATASET_FFB/SUHARJITO/NORMAL/DATASET_PHOTOMETRIC/test'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.0001\n",
        "momentum = 0.9\n",
        "batch_size = 8\n",
        "epochs = 20\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Data preprocessing\n",
        "train_datagen = ImageDataGenerator()\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Load EfficientNetB0 model\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze base model layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Build classification model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "output = Dense(6, activation='softmax')(x)  # 6 classes\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile model\n",
        "optimizer = SGD(learning_rate=learning_rate, momentum=momentum)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save('/content/drive/MyDrive/DATASET_FFB/SUHARJITO/NORMAL/DATASET_PHOTOMETRIC/efficientnetb0_kelapa_sawit_classification.keras')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "to2wB_ckvBjL",
        "outputId": "37c6d5ef-b1cb-44e3-8c5c-0e3c036a3d9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10710 images belonging to 6 classes.\n",
            "Found 3060 images belonging to 6 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 0s 0us/step\n",
            "Epoch 1/20\n",
            "1339/1339 [==============================] - 2746s 2s/step - loss: 1.3764 - accuracy: 0.5120 - val_loss: 1.1328 - val_accuracy: 0.6072\n",
            "Epoch 2/20\n",
            "1339/1339 [==============================] - 181s 135ms/step - loss: 1.0537 - accuracy: 0.6322 - val_loss: 0.9742 - val_accuracy: 0.6520\n",
            "Epoch 3/20\n",
            "1339/1339 [==============================] - 178s 133ms/step - loss: 0.9257 - accuracy: 0.6718 - val_loss: 0.8885 - val_accuracy: 0.6824\n",
            "Epoch 4/20\n",
            "1339/1339 [==============================] - 181s 135ms/step - loss: 0.8585 - accuracy: 0.6923 - val_loss: 0.8360 - val_accuracy: 0.7000\n",
            "Epoch 5/20\n",
            "1339/1339 [==============================] - 177s 132ms/step - loss: 0.8068 - accuracy: 0.7078 - val_loss: 0.7919 - val_accuracy: 0.7154\n",
            "Epoch 6/20\n",
            "1339/1339 [==============================] - 180s 134ms/step - loss: 0.7629 - accuracy: 0.7260 - val_loss: 0.7672 - val_accuracy: 0.7196\n",
            "Epoch 7/20\n",
            "1339/1339 [==============================] - 180s 134ms/step - loss: 0.7327 - accuracy: 0.7324 - val_loss: 0.7367 - val_accuracy: 0.7288\n",
            "Epoch 8/20\n",
            "1339/1339 [==============================] - 178s 133ms/step - loss: 0.7000 - accuracy: 0.7464 - val_loss: 0.7161 - val_accuracy: 0.7399\n",
            "Epoch 9/20\n",
            "1339/1339 [==============================] - 179s 133ms/step - loss: 0.6784 - accuracy: 0.7565 - val_loss: 0.6892 - val_accuracy: 0.7471\n",
            "Epoch 10/20\n",
            "1339/1339 [==============================] - 176s 132ms/step - loss: 0.6550 - accuracy: 0.7651 - val_loss: 0.6738 - val_accuracy: 0.7500\n",
            "Epoch 11/20\n",
            "1339/1339 [==============================] - 180s 134ms/step - loss: 0.6326 - accuracy: 0.7739 - val_loss: 0.6663 - val_accuracy: 0.7458\n",
            "Epoch 12/20\n",
            "1339/1339 [==============================] - 178s 133ms/step - loss: 0.6191 - accuracy: 0.7759 - val_loss: 0.6423 - val_accuracy: 0.7605\n",
            "Epoch 13/20\n",
            "1339/1339 [==============================] - 178s 133ms/step - loss: 0.6068 - accuracy: 0.7856 - val_loss: 0.6317 - val_accuracy: 0.7627\n",
            "Epoch 14/20\n",
            "1339/1339 [==============================] - 178s 133ms/step - loss: 0.5897 - accuracy: 0.7888 - val_loss: 0.6213 - val_accuracy: 0.7657\n",
            "Epoch 15/20\n",
            "1339/1339 [==============================] - 175s 131ms/step - loss: 0.5729 - accuracy: 0.7973 - val_loss: 0.6123 - val_accuracy: 0.7660\n",
            "Epoch 16/20\n",
            "1339/1339 [==============================] - 178s 133ms/step - loss: 0.5655 - accuracy: 0.7964 - val_loss: 0.6005 - val_accuracy: 0.7742\n",
            "Epoch 17/20\n",
            "1339/1339 [==============================] - 178s 133ms/step - loss: 0.5554 - accuracy: 0.8023 - val_loss: 0.5890 - val_accuracy: 0.7748\n",
            "Epoch 18/20\n",
            "1339/1339 [==============================] - 180s 134ms/step - loss: 0.5375 - accuracy: 0.8089 - val_loss: 0.5848 - val_accuracy: 0.7748\n",
            "Epoch 19/20\n",
            "1339/1339 [==============================] - 176s 132ms/step - loss: 0.5311 - accuracy: 0.8100 - val_loss: 0.5754 - val_accuracy: 0.7820\n",
            "Epoch 20/20\n",
            "1339/1339 [==============================] - 177s 132ms/step - loss: 0.5223 - accuracy: 0.8150 - val_loss: 0.5680 - val_accuracy: 0.7879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test data generator\n",
        "test_datagen = ImageDataGenerator()\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "-iglMbm9zHnQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3081ea9-c006-4962-cfeb-5da107bc4c31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1530 images belonging to 6 classes.\n",
            "192/192 [==============================] - 310s 2s/step - loss: 0.5233 - accuracy: 0.7974\n",
            "Test Accuracy: 79.74%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AUGMENTASI"
      ],
      "metadata": {
        "id": "kRgtrYAGNl1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV45DQQtNteR",
        "outputId": "c8549a1a-a395-4503-da49-189e15967285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import albumentations as A\n",
        "from albumentations.core.composition import Compose\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.0001\n",
        "momentum = 0.9\n",
        "batch_size = 8\n",
        "epochs = 150\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Directories\n",
        "train_dir = '/content/drive/MyDrive/DATASET_FFB/SUHARJITO/NORMAL/DATASET_BALANCED_SPLIT_MIN/train'\n",
        "val_dir = '/content/drive/MyDrive/DATASET_FFB/SUHARJITO/NORMAL/DATASET_BALANCED_SPLIT_MIN/valid'\n",
        "test_dir = '/content/drive/MyDrive/DATASET_FFB/SUHARJITO/NORMAL/DATASET_BALANCED_SPLIT_MIN/test'\n",
        "\n",
        "# Augmentation function\n",
        "def custom_augmentation(image):\n",
        "    # Define augmentations\n",
        "    transform = Compose([\n",
        "        A.RandomCrop(width=image_size[0] - 20, height=image_size[1] - 20, always_apply=True),  # Localization\n",
        "        A.Resize(height=image_size[0], width=image_size[1]),  # Resize after cropping\n",
        "        A.GaussianBlur(blur_limit=(3, 7), p=0.5),  # Gaussian Blur\n",
        "        A.Rotate(limit=40, p=1.0)  # 9-angle crop simulation through rotation\n",
        "    ])\n",
        "    augmented = transform(image=image)\n",
        "    return augmented['image'] / 255.0  # Normalize to [0, 1] range manually\n",
        "\n",
        "# Custom Data Generator with Albumentations\n",
        "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, directory, batch_size, image_size, aug_func, class_mode='categorical'):\n",
        "        self.directory = directory\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.aug_func = aug_func\n",
        "        self.class_mode = class_mode\n",
        "        self.class_indices = {v: k for k, v in enumerate(sorted(os.listdir(directory)))}\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        for class_name, idx in self.class_indices.items():\n",
        "            class_path = os.path.join(directory, class_name)\n",
        "            self.image_paths += [os.path.join(class_path, img) for img in os.listdir(class_path)]\n",
        "            self.labels += [idx] * len(os.listdir(class_path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.image_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        for img_path, label in zip(batch_x, batch_y):\n",
        "            image = load_img(img_path, target_size=self.image_size)\n",
        "            image = img_to_array(image)\n",
        "            image = self.aug_func(image)  # Apply augmentations without keyword argument\n",
        "            images.append(image)\n",
        "            labels.append(label)\n",
        "\n",
        "        images = np.array(images, dtype=\"float32\")\n",
        "        labels = tf.keras.utils.to_categorical(labels, num_classes=6)  # Adjust for 6 classes\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "# Initialize data generators\n",
        "train_generator = CustomDataGenerator(\n",
        "    directory=train_dir,\n",
        "    batch_size=batch_size,\n",
        "    image_size=image_size,\n",
        "    aug_func=custom_augmentation  # Augmentasi diterapkan pada data train\n",
        ")\n",
        "\n",
        "validation_generator = CustomDataGenerator(\n",
        "    directory=val_dir,\n",
        "    batch_size=batch_size,\n",
        "    image_size=image_size,\n",
        "    aug_func=lambda x: x / 255.0  # Hanya normalisasi, tanpa keyword argument\n",
        ")\n",
        "\n",
        "test_generator = CustomDataGenerator(\n",
        "    directory=test_dir,\n",
        "    batch_size=batch_size,\n",
        "    image_size=image_size,\n",
        "    aug_func=lambda x: x / 255.0  # Hanya normalisasi, tanpa keyword argument\n",
        ")\n",
        "\n",
        "# Load EfficientNetB0 model\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze base model layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Build classification model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "output = Dense(6, activation='softmax')(x)  # 6 classes\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile model\n",
        "optimizer = SGD(learning_rate=learning_rate, momentum=momentum)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save('/content/drive/MyDrive/path/to/saved_model/efficientnetb0_kelapa_sawit_classification_with_aug.keras')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-Xvlo_6No2t",
        "outputId": "8564edb8-fb42-4a8d-9c26-964fe41bb6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 677ms/step - accuracy: 0.1575 - loss: 1.7948 - val_accuracy: 0.1602 - val_loss: 1.7921\n",
            "Epoch 2/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 677ms/step - accuracy: 0.1878 - loss: 1.7909 - val_accuracy: 0.1699 - val_loss: 1.7920\n",
            "Epoch 3/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 674ms/step - accuracy: 0.1621 - loss: 1.7924 - val_accuracy: 0.1602 - val_loss: 1.7919\n",
            "Epoch 4/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 657ms/step - accuracy: 0.1505 - loss: 1.7933 - val_accuracy: 0.1680 - val_loss: 1.7919\n",
            "Epoch 5/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 652ms/step - accuracy: 0.1877 - loss: 1.7921 - val_accuracy: 0.1582 - val_loss: 1.7918\n",
            "Epoch 6/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 640ms/step - accuracy: 0.1456 - loss: 1.7929 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 7/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 652ms/step - accuracy: 0.1927 - loss: 1.7917 - val_accuracy: 0.1582 - val_loss: 1.7918\n",
            "Epoch 8/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 675ms/step - accuracy: 0.1648 - loss: 1.7932 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 9/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 675ms/step - accuracy: 0.1998 - loss: 1.7911 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 10/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 646ms/step - accuracy: 0.1746 - loss: 1.7922 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 11/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 656ms/step - accuracy: 0.1439 - loss: 1.7929 - val_accuracy: 0.1680 - val_loss: 1.7917\n",
            "Epoch 12/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 639ms/step - accuracy: 0.1306 - loss: 1.7935 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 13/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 639ms/step - accuracy: 0.1376 - loss: 1.7932 - val_accuracy: 0.1680 - val_loss: 1.7917\n",
            "Epoch 14/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 658ms/step - accuracy: 0.1573 - loss: 1.7924 - val_accuracy: 0.1602 - val_loss: 1.7918\n",
            "Epoch 15/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 639ms/step - accuracy: 0.1591 - loss: 1.7924 - val_accuracy: 0.1602 - val_loss: 1.7918\n",
            "Epoch 16/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 635ms/step - accuracy: 0.1595 - loss: 1.7922 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 17/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 638ms/step - accuracy: 0.1379 - loss: 1.7930 - val_accuracy: 0.1680 - val_loss: 1.7917\n",
            "Epoch 18/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 634ms/step - accuracy: 0.1509 - loss: 1.7929 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 19/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 635ms/step - accuracy: 0.1538 - loss: 1.7916 - val_accuracy: 0.1602 - val_loss: 1.7918\n",
            "Epoch 20/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 641ms/step - accuracy: 0.1774 - loss: 1.7920 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 21/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 626ms/step - accuracy: 0.1324 - loss: 1.7923 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 22/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 634ms/step - accuracy: 0.1687 - loss: 1.7924 - val_accuracy: 0.1680 - val_loss: 1.7917\n",
            "Epoch 23/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 633ms/step - accuracy: 0.1458 - loss: 1.7927 - val_accuracy: 0.1680 - val_loss: 1.7917\n",
            "Epoch 24/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 632ms/step - accuracy: 0.1367 - loss: 1.7936 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 25/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 641ms/step - accuracy: 0.1608 - loss: 1.7930 - val_accuracy: 0.1680 - val_loss: 1.7917\n",
            "Epoch 26/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 621ms/step - accuracy: 0.1659 - loss: 1.7923 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 27/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 618ms/step - accuracy: 0.1529 - loss: 1.7928 - val_accuracy: 0.1680 - val_loss: 1.7917\n",
            "Epoch 28/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 622ms/step - accuracy: 0.1604 - loss: 1.7927 - val_accuracy: 0.1680 - val_loss: 1.7917\n",
            "Epoch 29/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 645ms/step - accuracy: 0.1618 - loss: 1.7929 - val_accuracy: 0.1680 - val_loss: 1.7917\n",
            "Epoch 30/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 626ms/step - accuracy: 0.1612 - loss: 1.7923 - val_accuracy: 0.1680 - val_loss: 1.7917\n",
            "Epoch 31/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 629ms/step - accuracy: 0.1613 - loss: 1.7926 - val_accuracy: 0.1680 - val_loss: 1.7917\n",
            "Epoch 32/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 640ms/step - accuracy: 0.1567 - loss: 1.7923 - val_accuracy: 0.1816 - val_loss: 1.7917\n",
            "Epoch 33/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 625ms/step - accuracy: 0.1447 - loss: 1.7932 - val_accuracy: 0.1680 - val_loss: 1.7917\n",
            "Epoch 34/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 622ms/step - accuracy: 0.1669 - loss: 1.7925 - val_accuracy: 0.1680 - val_loss: 1.7917\n",
            "Epoch 35/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 623ms/step - accuracy: 0.1740 - loss: 1.7926 - val_accuracy: 0.1680 - val_loss: 1.7917\n",
            "Epoch 36/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 631ms/step - accuracy: 0.1768 - loss: 1.7916 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 37/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 625ms/step - accuracy: 0.1658 - loss: 1.7926 - val_accuracy: 0.1602 - val_loss: 1.7918\n",
            "Epoch 38/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 627ms/step - accuracy: 0.1691 - loss: 1.7918 - val_accuracy: 0.1602 - val_loss: 1.7918\n",
            "Epoch 39/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 620ms/step - accuracy: 0.1571 - loss: 1.7924 - val_accuracy: 0.1973 - val_loss: 1.7918\n",
            "Epoch 40/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 635ms/step - accuracy: 0.1585 - loss: 1.7928 - val_accuracy: 0.1602 - val_loss: 1.7918\n",
            "Epoch 41/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 618ms/step - accuracy: 0.1536 - loss: 1.7928 - val_accuracy: 0.1602 - val_loss: 1.7918\n",
            "Epoch 42/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 617ms/step - accuracy: 0.1485 - loss: 1.7931 - val_accuracy: 0.1602 - val_loss: 1.7918\n",
            "Epoch 43/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 633ms/step - accuracy: 0.1440 - loss: 1.7921 - val_accuracy: 0.1602 - val_loss: 1.7919\n",
            "Epoch 44/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 619ms/step - accuracy: 0.1497 - loss: 1.7928 - val_accuracy: 0.1758 - val_loss: 1.7918\n",
            "Epoch 45/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 622ms/step - accuracy: 0.1433 - loss: 1.7930 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 46/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 622ms/step - accuracy: 0.1569 - loss: 1.7927 - val_accuracy: 0.1602 - val_loss: 1.7918\n",
            "Epoch 47/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 632ms/step - accuracy: 0.1379 - loss: 1.7932 - val_accuracy: 0.1680 - val_loss: 1.7917\n",
            "Epoch 48/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 629ms/step - accuracy: 0.1589 - loss: 1.7924 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 49/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 625ms/step - accuracy: 0.1354 - loss: 1.7928 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 50/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 641ms/step - accuracy: 0.1864 - loss: 1.7921 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 51/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 626ms/step - accuracy: 0.1708 - loss: 1.7921 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 52/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 640ms/step - accuracy: 0.1461 - loss: 1.7930 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 53/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 621ms/step - accuracy: 0.1786 - loss: 1.7919 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 54/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 635ms/step - accuracy: 0.1477 - loss: 1.7927 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 55/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 624ms/step - accuracy: 0.1571 - loss: 1.7924 - val_accuracy: 0.1680 - val_loss: 1.7918\n",
            "Epoch 56/150\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563ms/step - accuracy: 0.1605 - loss: 1.7926"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Data Generator\n",
        "test_datagen = CustomDataGenerator(\n",
        "    directory=test_dir,\n",
        "    batch_size=batch_size,\n",
        "    image_size=image_size,\n",
        "    aug_func=lambda x: x / 255.0  # No augmentations, only normalization\n",
        ")\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model('efficientnetb0_kelapa_sawit_classification_with_aug.keras')\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(test_datagen)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Test Loss: {test_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "SVdhF0GaOdML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model('efficientnetb0_kelapa_sawit_classification_with_aug.keras')\n",
        "\n",
        "# Define custom generators for training, validation, and test data\n",
        "train_datagen = CustomDataGenerator(\n",
        "    directory=train_dir,\n",
        "    batch_size=batch_size,\n",
        "    image_size=image_size,\n",
        "    aug_func=lambda x: x / 255.0  # No augmentations, only normalization\n",
        ")\n",
        "\n",
        "validation_datagen = CustomDataGenerator(\n",
        "    directory=val_dir,\n",
        "    batch_size=batch_size,\n",
        "    image_size=image_size,\n",
        "    aug_func=lambda x: x / 255.0\n",
        ")\n",
        "\n",
        "test_datagen = CustomDataGenerator(\n",
        "    directory=test_dir,\n",
        "    batch_size=batch_size,\n",
        "    image_size=image_size,\n",
        "    aug_func=lambda x: x / 255.0\n",
        ")\n",
        "\n",
        "# Evaluate on train, validation, and test sets\n",
        "train_loss, train_accuracy = model.evaluate(train_datagen)\n",
        "val_loss, val_accuracy = model.evaluate(validation_datagen)\n",
        "test_loss, test_accuracy = model.evaluate(test_datagen)\n",
        "\n",
        "# Tabel Akurasi\n",
        "accuracy_df = pd.DataFrame({\n",
        "    \"Dataset\": [\"Train\", \"Validation\", \"Test\"],\n",
        "    \"Loss\": [train_loss, val_loss, test_loss],\n",
        "    \"Accuracy\": [train_accuracy, val_accuracy, test_accuracy]\n",
        "})\n",
        "\n",
        "# Display the accuracy table\n",
        "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Akurasi Dataset\", dataframe=accuracy_df)\n",
        "\n",
        "# Confusion Matrix for Test Set\n",
        "# Generate predictions and true labels\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "for images, labels in test_datagen:\n",
        "    predictions = model.predict(images)\n",
        "    y_pred.extend(np.argmax(predictions, axis=1))\n",
        "    y_true.extend(np.argmax(labels, axis=1))\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "class_labels = list(test_datagen.class_indices.keys())\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix for Test Set\")\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_true, y_pred, target_names=class_labels)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "id": "gFGV8aSYOvQX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}